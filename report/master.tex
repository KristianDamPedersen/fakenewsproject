\documentclass{article}

\include{preamble}

\begin{document}
\maketitle


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}



\section{Data Processing}
\subsection{Sample data}
We downloaded the sample data and loaded it into a pandas dataframe, which did not pose any challenges. 
We then performed tokenization, stopword removal, lowercasing and stemming (see notebook for details). Our processing revealed:
\begin{table}[h]
    \centering
    \begin{tabular}{r| c | c | c| c}
      Data& vocabulary size & vocabulary size (lowered) & \% decrease & \% decrease (lowered)\\
        \hline
      Sample data& 21016 & 18011 & 0\% & 0\% \\
    \hline
      Removed stopwords & 20674 & 17865 & 1.63\% & 0.81\% \\
    \hline
      Stemmed & 12654 & 12703 & 39.8\% & 29.5\%
    \end{tabular}
\end{table}
% TODO ADD THESE NUMBERS TO TABLE

\label{sec:headings}
\subsection{Getting the data}
The first challenge was simply downloading and unpacking the files on our Linux systems. Using different GUI tools failed, and in the end, the method that worked was simply concatinating the files into a temporary zip file using \texttt{cat}, and then unzipping the combined file using \texttt{unzip}.\\
\\
Trying to read the data in python proved to be a challenging task, as the .csv file seemed to be corrupted or malformed with rows containing different amounts of columns. After some manual exploration, we found the problem which turned out to be carriage returns. Some article content included \texttt{\textbackslash r}, which was interpreted as "newline", and thus, a new row in the middle of arbitrary news content. A single command '\texttt{sed 's/\textbackslash r/ /g' in.csv > out.csv}' replacing the carriage returns with spaces (to preserve word boundaries) turned out to solve this issue.\\

\subsection{Working with larger than memory datasets}
One of the main hurdles to overcome when gathering and processing the data, was the fact that the dataset was larger
than our available RAM. This prevented us from loading all of the dataset into memory (as required if using Pandas
dataframes) at any one time. To overcome this limitation we chose to first revise the format of our data. We decided to
convert the large CSV file into the more managable \textit{parquet} format (see jupyter notebook for conversion script). The advantage of parquet files, is that they
come with detailed associated meta-data. In the meta-data we were able to split our dataset into chunks, which we could
then use to sequentially load the dataset into memory in bytes, and put it back together correctly.

Pandas unfortunately doesn't support this sequential streaming into memory, so we decided to switch to a library called
\textit{Dask}. Dask is designed around working with larger than memory datasets, and leverage an advanced scheduler to both
enable chunk-based operations, lazy calculations and even multithreading and distributing work across computers. This makes working with large data much easier.
\subsection{Processing with Dask}
Things to describe:
\begin{itemize}
    \item Tokenize the text.
    \item Remove stopwords and compute the size of the vocabulary. Compute the reduction rate of the vocabulary size after removing stopwords.
    \item Remove word variations with stemming and compute the size of the vocabulary. Compute the reduction rate of the vocabulary size after stemming.
\end{itemize}
    
    
    

\subsection{Some non-trivial observations}
When exploring our data, we found the following problems:
\subsubsection{Carriage returns}
As previously mentioned, before the csv was even usable, we had to remove carriage returns from the news content.
\subsubsection{Wikileaks}
While manually inspecting the data, we randomly noticed two articles containing the same source
\begin{quote}
    \textit{Tor}

    \textit{Tor is an encrypted anonymising network that makes it harder to} [\dots]\\
    \textit{In order to use the WikiLeaks public submission system as detailed above you can download the Tor Browser} [\dots]
\end{quote}
Further investigation revealed, that 160,880 out of the 199,030 meaning 80.83\% of all articles from \emph{wikileaks.org} had this exact content. (see jupyter notebook for investigation script). Given that our corpus contains 8,528,956 articles, this error alone makes um 1.9\% of the full corpus. In no way a trivial amount.
\subsubsection{Missing or inconsistent values}
Some columns are largely unused, and some important columns have arbitrary empty fields. Notably, the "Type" column being the most important for our task of classifying the news articles is sometimes empty. There also exist two "Types" not specified in the source documentation for the dataset. The additional types being \emph{unknown} and \emph{rumor}.
\subsubsection{Typos per domain???}
.,.,.,.,.,.,.,
\subsubsection{Other fun metrics????}
.,.,.,.,.,.,.,.
\newpage

\section{Simple model (1/2 page)} 
\todo{Difficult to decide on exactly what we should include here with only 1/2 page..}
In this section we go the logistic regression model we created, to serve as a baseline for comparison for the more
complex model we discuss in section \ref{sec:complex_model}. We will go over how we prepared the data for the model,
including grouping of article types, TFIDF feature extraction and dimensionality reduction. We will then go over the
performance metrics we've chosen, as well as the specific hyper-parameters used in the model. Finally we will analyze
the result and comment on potential shortcomings of the model. 
\subsection{Preparing data for the simple model}
\subsubsection{What features to focus on}
We chose to only be concerned with the "content" column for the base from which we want to predict the type. Whilst it
might be tempting to also include features such as "domain" as input, we found that this leads to massive over-fitting,
primarily because the dataset is constructed by labeling articles as true or false based on their domain (i.e. all
articles from the Washington Post being labeled true, versus all from Fox News being labeled as false) (**REF**).
\subsubsection{Grouping}
Whilst it is possible to use logistic regression for multi-class classification, we are in this case only interested in
a binary scenario of "fake news" or "reliable news". We therefore decided to encode the "type" of the article into 1's
and 0's, such that \textit{reliable} is encoded as 1 and the rest is encoded as 0. We've chosen this rather
conservative approach to what counts as reliable (ex. excluding "political" articles from being reliable), because it
would be of greater ethical concern if we labelled a fake article as true by mistake versus labeling a true article
fake by mistake.

\subsubsection{TF-IDF feature extraction}
In order to extract useful features from the content column, we decided to run it through a TF-IDF (Term
Frequency - Inverse Document Frequency) model in order to identify the importance of different words for predicting
wether an article was reliable or not. We can then use the weights gained through the TF-IDF model as our input vector
in our logistic regression model. \todo{Maybe add the math behind TF-IDF and / or potentially the tokenization we did}

\subsubsection{Dimensionality reduction through Truncated Singular Value Decomposition}
After running the content through the TF-IDF model, we were left with a rather large input vector with a dimensionality
of2048***
\todo{Correct?}. In order to consolidate similar features together, and to improve the performance we reduced the
dimensionality of our input using truncated singular value decomposition (REF) \todo{Remember to add reference}.


\subsection{Model}
After preparing the data for training, we created our logistic regression model. When creating a logistic regression
model, there are a few important hyper-parameters to consider. First we have the choice of solver. 
\subsubsection{Hyper-parameters}


\subsection{Limitations to our grouping of classes}
We decided to only keep the "reliable" and "fake" classes from the dataset and disregarded everything else. This gave us a dataset that was still about 2 million rows which is already too big to train anything on. Potential limitations are that we don't have as many domains and we don't have a big diverse range of article types.


\section{Complex model (1 page)}\label{sec:complex_model}
\subsection{Preparing data for the complex model}

\subsection{Model}

\subsection{Results}

\subsection{Short-comings}

\subsection{Evaluation}
In this section we will go over the performance on our two models on the FakeNewsCorpus dataset. We start by going over
our evaluation metrics, namely F-Score. We will then go over the results of our models, and how these compare to
eachother. Lastly we will go over the potential shortcomings of our approach, as we'll as comment on the general
challenges of creating fake news classifiers.

\section{Evaluation metrics}

\subsection{Results and comparison}

\subsection{Short-comings and ethical challenges}

\section{Conclusion}

\subsection{Citations}
Citations use \verb+natbib+. The documentation may be found at
\begin{center}
	\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}

Here is an example usage of the two main commands (\verb+citet+ and \verb+citep+): Some people thought a thing \citep{kour2014real, hadash2018estimate} but other people thought something else \citep{kour2014fast}. Many people have speculated that if we knew exactly why \citet{kour2014fast} thought this\dots

\subsection{Figures}
\lipsum[10]
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11]

\begin{figure}
	\centering
	\fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
	\caption{Sample figure caption.}
	\label{fig:fig1}
\end{figure}

\subsection{Tables}
See awesome Table~\ref{tab:table}.

The documentation for \verb+booktabs+ (`Publication quality tables in LaTeX') is available from:
\begin{center}
	\url{https://www.ctan.org/pkg/booktabs}
\end{center}


\begin{table}
	\caption{Sample table title}
	\centering
	\begin{tabular}{lll}
		\toprule
		\multicolumn{2}{c}{Part}                   \\
		\cmidrule(r){1-2}
		Name     & Description     & Size ($\mu$m) \\
		\midrule
		Dendrite & Input terminal  & $\sim$100     \\
		Axon     & Output terminal & $\sim$10      \\
		Soma     & Cell body       & up to $10^6$  \\
		\bottomrule
	\end{tabular}
	\label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
	\item Lorem ipsum dolor sit amet
	\item consectetur adipiscing elit.
	\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\bibliographystyle{unsrtnat}
\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .


%%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
% \begin{thebibliography}{1}

% 	\bibitem{kour2014real}
% 	George Kour and Raid Saabne.
% 	\newblock Real-time segmentation of on-line handwritten arabic script.
% 	\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
% 			International Conference on}, pages 417--422. IEEE, 2014.

% 	\bibitem{kour2014fast}
% 	George Kour and Raid Saabne.
% 	\newblock Fast classification of handwritten on-line arabic characters.
% 	\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
% 			International Conference of}, pages 312--318. IEEE, 2014.

% 	\bibitem{hadash2018estimate}
% 	Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
% 	Jacovi.
% 	\newblock Estimate and replace: A novel approach to integrating deep neural
% 	networks with existing applications.
% 	\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}


\end{document}
