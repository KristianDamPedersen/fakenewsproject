\documentclass{article}

\include{preamble}

\begin{document}
\maketitle


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}



\section{Data Processing}
\subsection{Sample data}
We downloaded the sample data and loaded it into a pandas dataframe, which did not pose any challenges. 
We then performed tokenization, stopword removal and stemming (see notebook for details). Our processing revealed:
\begin{table}[h]
    \centering
    \begin{tabular}{r| c | c | c| c}
      Data& vocabulary size & vocabulary size (lowered) & \% decrease & \% decrease (lowered)\\
        \hline
      Sample data& 21016 & 18011 & 0\% & 0\% \\
    \hline
      Removed stopwords & 20674 & 17865 & 1.63\% & 0.81\% \\
    \hline
      Stemmed & 12654 & 12703 & 39.8\% & 29.5\%
    \end{tabular}
\end{table}
% TODO ADD THESE NUMBERS TO TABLE

\label{sec:headings}
\subsection{Getting the data}
The first challenge was simply downloading and unpacking the files on our Linux systems. Using different GUI tools failed, and in the end, the method that worked was simply concatinating the files into a temporary zip file using \texttt{cat}, and then unzipping the combined file using \texttt{unzip}.\\
\\
Trying to read the data in python proved to be a challenging task, as the .csv file seemed to be corrupted with rows containing different amounts of columns. After some manual exploration, we found the problem which turned out to be carriage returns. Some article content included \texttt{\textbackslash r}, which was interpreted as "newline", and thus, a new row in the middle of arbitrary news content. A single command '\texttt{sed 's/\textbackslash r/ /g' in.csv > out.csv}' replacing the carriage returns with spaces (to preserve word boundaries) turned out to solve this issue.\\


\subsection{Working with larger than memory datasets}
One of the main hurdles to overcome when gathering and processing the data, was the fact that the dataset was larger
than our available RAM. This prevented us from loading all of the dataset into memory (as required if using Pandas
dataframes) at any one time. To overcome this limitation we chose to first revise the format of our data. We decided to
convert the large CSV file into the more managable \textit{parquet} format (see jupyter notebook for conversion script). The advantage of parquet files, is that they
come with detailed associated meta-data. In the meta-data we were able to split our dataset into chunks, which we could
then use to sequentially load the dataset into memory in bites, and put it back together correctly.

Pandas unfortunately doesn't support this sequential streaming into memory, so we decided to switch to a library called
\textit{Dask}. Dask is designed around working with larger than memory datasets, and leverage an advanced scheduler to both
enable chunk-based operations, lazy calculations and even multithreading and distributing work across computers. This makes working with large data much easier.
\subsection{Processing with Dask}
Things to describe:
\begin{itemize}
    \item Tokenize the text.
    \item Remove stopwords and compute the size of the vocabulary. Compute the reduction rate of the vocabulary size after removing stopwords.
    \item Remove word variations with stemming and compute the size of the vocabulary. Compute the reduction rate of the vocabulary size after stemming.
\end{itemize}
    
    
    

\subsection{Some non-trivial observations}
When exploring our data, we found the following problems:
\subsubsection{Carriage returns}
As previously mentioned, before the csv was even usable, we had to remove carriage returns from the news content.
\subsubsection{Wikileaks}
While manually inspecting the data, we randomly noticed two articles containing the same source
\begin{quote}
    \textit{Tor}

    \textit{Tor is an encrypted anonymising network that makes it harder to} [\dots]\\
    \textit{In order to use the WikiLeaks public submission system as detailed above you can download the Tor Browser} [\dots]
\end{quote}
Further investigation revealed, that 160,880 out of the 199,030 meaning 80.83\% of all articles from \emph{wikileaks.org} had this exact content. (see jupyter notebook for investigation script). Given that our corpus contains 8,528,956 articles, this error alone makes um 1.9\% of the full corpus. In no way a trivial amount.
\subsubsection{Missing or inconsistent values}
Some columns are largely unused, and some important columns have arbitrary empty fields. Notably, the "Type" column being the most important for our task of classifying the news articles is sometimes empty. There also exist two "Types" not specified in the source documentation for the dataset. The additional types being \emph{unknown} and \emph{rumor}.
\subsubsection{Typos per domain???}
.,.,.,.,.,.,.,
\subsubsection{Other fun metrics????}
.,.,.,.,.,.,.,.
\newpage

\subsection{Citations}
Citations use \verb+natbib+. The documentation may be found at
\begin{center}
	\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}

Here is an example usage of the two main commands (\verb+citet+ and \verb+citep+): Some people thought a thing \citep{kour2014real, hadash2018estimate} but other people thought something else \citep{kour2014fast}. Many people have speculated that if we knew exactly why \citet{kour2014fast} thought this\dots

\subsection{Figures}
\lipsum[10]
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11]

\begin{figure}
	\centering
	\fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
	\caption{Sample figure caption.}
	\label{fig:fig1}
\end{figure}

\subsection{Tables}
See awesome Table~\ref{tab:table}.

The documentation for \verb+booktabs+ (`Publication quality tables in LaTeX') is available from:
\begin{center}
	\url{https://www.ctan.org/pkg/booktabs}
\end{center}


\begin{table}
	\caption{Sample table title}
	\centering
	\begin{tabular}{lll}
		\toprule
		\multicolumn{2}{c}{Part}                   \\
		\cmidrule(r){1-2}
		Name     & Description     & Size ($\mu$m) \\
		\midrule
		Dendrite & Input terminal  & $\sim$100     \\
		Axon     & Output terminal & $\sim$10      \\
		Soma     & Cell body       & up to $10^6$  \\
		\bottomrule
	\end{tabular}
	\label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
	\item Lorem ipsum dolor sit amet
	\item consectetur adipiscing elit.
	\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\bibliographystyle{unsrtnat}
\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .


%%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
% \begin{thebibliography}{1}

% 	\bibitem{kour2014real}
% 	George Kour and Raid Saabne.
% 	\newblock Real-time segmentation of on-line handwritten arabic script.
% 	\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
% 			International Conference on}, pages 417--422. IEEE, 2014.

% 	\bibitem{kour2014fast}
% 	George Kour and Raid Saabne.
% 	\newblock Fast classification of handwritten on-line arabic characters.
% 	\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
% 			International Conference of}, pages 312--318. IEEE, 2014.

% 	\bibitem{hadash2018estimate}
% 	Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
% 	Jacovi.
% 	\newblock Estimate and replace: A novel approach to integrating deep neural
% 	networks with existing applications.
% 	\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}


\end{document}
