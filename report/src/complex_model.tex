\section{Complex models}
\subsection{XGBoost}
This model starts out with the data piped through TF-IDF 4096 and SVD-386. XGBoost is an ensemble learner, meaning it is a collection of smaller models. The way this algorithm is trained, is we start with a decision tree. Then in every boosting round (we do this 10 times), we find the weights responsible for incorrect classificatins, boost them (making them more important for the specific tree) and try to minimise the loss. This means every subsequent tree is trying to correct the errors of the previous tree.

\subsection{DNN 1}
This model uses TF-IDF 4096 without further dimensionality reduction. It a simple feed-forward network with the layers 1024, DO, 512, DO, 256, DO, 128, DO, 64, 1 where DO is a drop-out layer of 0.2, meaning every epoch it resets 20\% of the weights. The idea with this funnel-shaped network was that it would reduce the dimensions on its own and learn how to classify the dataset with minimal loss of data. Shortcomings of the model: The model is on the larger side (57 MB), was slow to train and has a slow inference speed.

\subsection{DNN 2}
This model starts out with the data piped through TF-IDF 4096 and SVD-386. This model is a more traditionally shaped feed-forward network and is the spiritual successor to the first one. Since the data was passed through dimsionality reduction already, it means the initial input space can be smaller. The layers are: 384, DO, 512, DO, 512, DO, 128, DO', 16, 1. DO' is a dropout of 0.1 and DO is the same as in DNN 1. This models performance is insignificantly lower than DNN 1 (<1\%), but is much smaller, faster to train and has a faster inference speed.
