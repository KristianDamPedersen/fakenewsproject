\section{Conclusion}
In this technical report we described the exploration of three different models for fake news classification. We started
by outlining the data import, exploration and processing we applied to the FakeNewsCorpus dataset. The processing
included both tokenization, feature extracting using TF-IDF and dimensionality reduction through single value
decomposition. We build a logistic regression model to serve as our baseline, and then constructed three more complex
models (two DNNs and an XGBoost model). Whilst all the models performed admirably on the FakeNewsCorpus dataset, they
all failed to generalize when we tested them against another dataset (LIAR). As discussed in our evaluation section,
this is likely due both to the quality of our dataset and the inherent difficulties in constructing fake news
classifiers in general. In the end we conclude that our approach has failed to build a general fake news classifier, and
that more sophisticated tools needs to be incorporated in order to achieve generalisable success.

