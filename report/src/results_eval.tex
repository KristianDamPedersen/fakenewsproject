\section{Results and evaluation}
In this section we will discuss the performance of both our simple and complex models, and discuss how these stack up
against eachother. We will discuss the accuracy and performance on both the FakeNewsCorpus dataset and the LIAR
dataset. And we will end of this evaluation by discussing the challenges in general fake news classifiers, and the
ethical and practical challenges such a model encounters.

\subsection{Relative performance on FakeNewsCorpus}
Let's begin by looking at our models performance on the FakeNewsCorpus dataset. As we see from table \ref{tab:fakenewsperformance},
all of our models performed admirably when being run against the test set, with our more complex models pulling ahead
comfortably.

\begin{table}[htpb]
  \centering
  \caption{Model performance on FakeNewsCorpus dataset}
  \label{tab:fakenewsperformance}

  \begin{tabular}{c|cccc}
    Model & Logistic Regression & Large DNN & Small DNN & XGBoost \\ \hline
    Precision (Reliable / Unreliable) & 0.95 / 0.98 & 0.83 / 0.94 & 0.92 / 0.98 & 0.87 / 0.95 \\ \hline
    Recall (Reliable / Unreliable) & 0.95 / 0.98 & 0.87 / 0.92 & 0.95 / 0.96 & 0.89 / 0.94 \\ \hline
    F1-Score (Reliable / Unreliable) & 0.95 / 0.98 & 0.85 / 0.93 & 0.93 / 0.97 & 0.88 / 0.95 \\ \hline
    Size & 16.6 MB & 121 MB & 34 MB & 29.8 MB 
  \end{tabular}
\end{table}

\subsection{Generalisability and why the models are shit at LIAR}
However, whilst these results are impressive, they also hint that our models might be overfitted to our domain. This is
likely due to the nature of the dataset we used for training not being general enough, which means a broader selection
of data sources and data sets would be needed to achieve a more generalisable model. This issue is highlighted in
particular by the relatively terrible accuracy acheived on the LIAR dataset as seen in table \ref{tab:liarperformance}.

\begin{table}[htpb]
  \centering
  \caption{Model performance on LIAR dataset}
  \label{tab:liarperformance}
  \begin{tabular}{c|cccc}
    Model & Logistic Regression & Large DNN & Small DNN & XGBoost \\ \hline
    Precision (True / False) & 0.17 / 0.84  & 0.18 / 0.84 & 0.18 / 0.84 & 0.17 / 0.84 \\ \hline
    Recall (True / False) & 0.13 / 0.88 & 0.07 / 0.94 & 0.12 / 0.89 & 0.19 / 0.83 \\ \hline
    F1-Score (True / False) & 0.14 / 0.86 & 0.10 / 0.89 & 0.14 / 0.87 & 0.18 / 0.84 \\ \hline
  \end{tabular}
\end{table}

\subsubsection{ROC comparison}
We decided to visualize the performance using ROC comparison, and as we see from figure (REF), all of the models lose
(almost) all their predictive power when we apply them to the LIAR dataset.


\subsubsection{UMAP and lack of seperability in LIAR}
As we did in the beginning, we decided to put the LIAR dataset through the UMAP model that was fitted to the
FakeNewsCorpus dataset. From figure (REF) we see that the articles in the LIAR dataset completely lacks seperability,
meaning our models have no real way of delineating them from eachother.


\subsection{Ethical considerations with fake news classification}
The subpar performance on the LIAR dataset highlights the difficulty in creating a proper fake news classifier. When trying to
generalise a model, questions about what consitutes reliable, fake or misleading news quickly arises. Does a publication raising awareness
of the real side-effects of the HPV vaccine count as reliable? And how does an algorithm distinguish this from a
publication spreading fake statistics of the corona vaccine?. Whilst there are publishers that we all (generally)
accept as being reliable, and others that are generally percieved as being unreliable, the exact line delineating the
two is hard (if not impossible) to strongly define.
