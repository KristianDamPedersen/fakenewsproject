\section{Results and evaluation}
In this section we will discuss the performance of both our simple and complex models, and discuss how these stack up
against eachother. We will discuss the accuracy and performance on both the FakeNewsCorpus dataset and the LIAR
dataset. And we will end this evaluation by discussing the challenges in general fake news classifiers, and the
ethical and practical challenges such a model encounters.

\subsection{Relative performance on FakeNewsCorpus}
Let's begin by looking at our models performance on the FakeNewsCorpus dataset. As we see from table \ref{tab:fakenewsperformance},
all of our models performed admirably when put against the test set, with our more complex models pulling ahead
comfortably.

\begin{table}[htpb]
  \centering
  \caption{Model performance on FakeNewsCorpus dataset}
  \label{tab:fakenewsperformance}

  \begin{tabular}{c|cccc}
    Model & Logistic Regression & Large DNN & Small DNN & XGBoost \\ \hline
    Precision (Reliable / Unreliable) & 0.83 / 0.94 & 0.95 / 0.98 & 0.92 / 0.98 & 0.87 / 0.95 \\ \hline
    Recall (Reliable / Unreliable) & 0.87 / 0.92 & 0.95 / 0.92 & 0.95 / 0.96 & 0.89 / 0.94 \\ \hline
    F1-Score & 0.91 & 0.97 & 0.96 & 0.93 \\ \hline
    Size & 15.6 MiB & 71 MiB & 35 MiB & 27.1 MiB 
  \end{tabular}
\end{table}

\subsection{Generalisability to other datasets (LIAR)}
However, whilst these results are impressive, they also hint that our models might be overfitted to our domain. This is
likely due to the nature of the dataset we used for training not being general enough, which means a broader selection
of data sources and data sets would be needed to achieve a more generalisable model. This issue is highlighted in
particular by the relatively terrible accuracy achieved on the LIAR dataset as seen in table \ref{tab:liarperformance}.

\begin{table}[htpb]
  \centering
  \caption{Model performance on LIAR dataset}
  \label{tab:liarperformance}
  \begin{tabular}{c|cccc}
    Model & Logistic Regression & Large DNN & Small DNN & XGBoost \\ \hline
    Precision (True / False) & 0.17 / 0.84  & 0.18 / 0.84 & 0.18 / 0.84 & 0.17 / 0.84 \\ \hline
    Recall (True / False) & 0.13 / 0.88 & 0.07 / 0.94 & 0.12 / 0.89 & 0.19 / 0.83 \\ \hline
    F1-Score& 0.76 & 0.80 & 0.77 & 0.73 \\
  \end{tabular}
\end{table}

\subsubsection{Performance visualisation}
These different metrics can be difficult to parse when evaluating on unbalanced datasets. This is where visual aids can
come in handy. Confusion matrices and ROC curves effectively show how much our models struggled with predictions outside
their native domain.The confusion matrices are split up into two columns and two rows. For a perfect model we want the
matrix to be "chequered" such that its perfect at predicting true positives and true negatives. However as we see just
columns, it means that our models have zero predictive power. This sentiment is echoed by the ROC plots.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/matrix_combined}
  \caption{Confusion matrices for  models predicting on LIAR dataset}
  \label{fig:conf_mat}
\end{figure}
\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/ROC_combined}
  \caption{ROC comparison - FakeNewsCorpus and LIAR}
  \label{fig:roc}
\end{figure}


\subsubsection{Inseparability of LIAR on fitted models}
We ran LIAR through the same TF-IDF, SVD and UMAP pipeline to see where they were located in the clusters, we would hope
to see some sort of separation of the various levels of truthiness if we want to have any hope in predicting truthiness
of the LIAR dataset. As seen in the UMAP representation (figure \ref{fig:liarvsfake}), the 6 classes of LIAR are interleaved, which means we will not be able to separate the classes nor make any meaningful inferences on the dataset.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/umapLiarVsFake}
  \caption{UMAP - LIAR overlayed on FakeNewsCorpus}
  \label{fig:liarvsfake}
\end{figure}

