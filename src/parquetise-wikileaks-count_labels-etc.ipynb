{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb7d7f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 0852"
     ]
    }
   ],
   "source": [
    "# Convert the csv to many parquet files, each containing 10000 rows\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "csv_file_path = 'cleaned_file.csv'\n",
    "parquet_file_prefix = 'cleaned_file.parquet/file_'\n",
    "parquet_file_suffix = '.parquet'\n",
    "\n",
    "chunk_size = 10000  # number of rows per chunk\n",
    "\n",
    "# Iterate over the CSV file in chunks and write each chunk to a separate Parquet file\n",
    "for i, chunk in enumerate(pd.read_csv(csv_file_path, chunksize=chunk_size)):\n",
    "    # Create the filename for the current chunk\n",
    "    ident = str(i).zfill(4)\n",
    "    parquet_file_path = parquet_file_prefix + ident + parquet_file_suffix\n",
    "\n",
    "    # Write the current chunk to a Parquet file\n",
    "    table = pa.Table.from_pandas(chunk)\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "    print(\"\\rchunk\", ident, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb28117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikileaks_problem(colnum, path='cleaned_file.parquet/'):\n",
    "    import pyarrow.parquet as pq\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    \n",
    "    text = 'Tor\\n\\nTor is an encrypted anonymising network that makes it harder to intercept '\n",
    "    text += 'internet communications, or see where communications are coming from or going '\n",
    "    text += 'to.\\n\\nIn order to use the WikiLeaks public submission system as detailed above '\n",
    "    text += 'you can download the Tor Browser Bundle, which is a Firefox-like browser available '\n",
    "    text += 'for Windows, Mac OS X and GNU/Linux and pre-configured to connect using the anonymising '\n",
    "    text += 'system Tor.\\n\\nTails\\n\\nIf you are at high risk and you have the capacity to do so, '\n",
    "    text += 'you can also access the submission system through a secure operating system called '\n",
    "    text += 'Tails. Tails is an operating system launched from a USB stick or a DVD that aim to '\n",
    "    text += 'leaves no traces when the computer is shut down after use and automatically routes '\n",
    "    text += 'your internet traffic through Tor. Tails will require you to have either a USB stick '\n",
    "    text += 'or a DVD at least 4GB big and a laptop or desktop computer.'\n",
    "    \n",
    "    \n",
    "    onlyparquets = [f for f in listdir(path) if isfile(join(path, f)) and f.endswith('.parquet')]\n",
    "\n",
    "    \n",
    "    column_name = 'domain'\n",
    "    search_value = 'wikileaks.org'\n",
    "    matches = articles = 0\n",
    "    \n",
    "    filter_condition = (column_name, '==', search_value)\n",
    "    \n",
    "    for file in onlyparquets:\n",
    "        print(('checking '+file).ljust(27), f'{matches}/{articles} = ', round(100*matches/max(articles,1),2),'%','        \\r', end='')\n",
    "        column_names = parquet_file.schema.names\n",
    "        table = pq.read_table(path+file, filters=[filter_condition])\n",
    "        for news in table[colnum]:\n",
    "            if str(news) == text:\n",
    "                matches += 1\n",
    "            articles += 1\n",
    "    return (matches,articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = wikileaks_problem(1, path='joshdata/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37c3be5",
   "metadata": {},
   "source": [
    "output: `checking chunk_684.parquet  160880/199030 =  80.83 %`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f63f0a",
   "metadata": {},
   "source": [
    "Get amounts of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "025062f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(path='cleaned_file.parquet/'):\n",
    "    import pyarrow.parquet as pq\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    \n",
    "    onlyparquets = [f for f in listdir(path) if isfile(join(path, f)) and f.endswith('.parquet')]\n",
    "\n",
    "    \n",
    "    column_name = 'label'\n",
    "    #search_value = 'wikileaks.org'\n",
    "    labelcount = {}\n",
    "    n = 0\n",
    "    #filter_condition = (column_name, '==', search_value)\n",
    "    \n",
    "    for file in onlyparquets:\n",
    "        print('checking '+file,'\\r', end='')\n",
    "        table = pq.read_table(path+file)\n",
    "        for row in table['type']:\n",
    "            this_type = row.as_py()\n",
    "            if not this_type:\n",
    "                this_type = 'empty'\n",
    "            if this_type in labelcount:\n",
    "                labelcount[this_type] += 1\n",
    "            else:\n",
    "                labelcount[this_type] = 1\n",
    "        n += 1\n",
    "    print()\n",
    "    return labelcount\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05810a86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking file_0509.parquet \n",
      "unknown     :     371518\n",
      "bias        :    1138998\n",
      "fake        :     894746\n",
      "political   :    1657224\n",
      "empty       :     403211\n",
      "rumor       :     481158\n",
      "conspiracy  :     831235\n",
      "clickbait   :     231949\n",
      "reliable    :    1913222\n",
      "satire      :     112948\n",
      "unreliable  :     298784\n",
      "junksci     :     117467\n",
      "hate        :      76496\n"
     ]
    }
   ],
   "source": [
    "labels = count_labels()\n",
    "for key, value in labels.items():\n",
    "    print(\"{:<12}: {:>10}\".format(key, value),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e29a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b325cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26baadb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f795e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8432a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edadcfeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a6fcb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3158fd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296411a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aee01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1b020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
