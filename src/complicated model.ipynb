{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0db1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put id, type, column in temp database (when fake or reliable)\n",
    "import sqlite3\n",
    "import csv\n",
    "import random\n",
    "n = 0\n",
    "n_inserted = 0\n",
    "\n",
    "input_file = \"cleaned_file.csv\"\n",
    "#output_file = \"cleaned_file_shuffled.csv\"\n",
    "db_file = \"temp_database.db\"\n",
    "\n",
    "# Connect to the SQLite database file\n",
    "conn = sqlite3.connect(db_file)\n",
    "c = conn.cursor()\n",
    "c.execute(\"CREATE TABLE data (id TEXT, type TEXT, content TEXT, title TEXT);\")\n",
    "\n",
    "# Read the input file and insert rows into the SQLite database\n",
    "with open(input_file, \"r\") as f_in:\n",
    "    reader = csv.reader(f_in)\n",
    "    header = next(reader)  # Read the header\n",
    "\n",
    "    for row in reader:\n",
    "        row_type = row[3]\n",
    "        if row_type == \"fake\" or row_type == \"reliable\":\n",
    "            # Remove newlines from each value in the row\n",
    "            id_value = row[1].replace(\"\\n\", \" \")\n",
    "            type_value = row[3].replace(\"\\n\", \" \")\n",
    "            content_value = row[5].replace(\"\\n\", \" \")\n",
    "            title_value = row[9].replace(\"\\n\", \" \")\n",
    "\n",
    "            c.execute(\"INSERT INTO data (id, type, content, title) VALUES (?, ?, ?, ?)\", (id_value, type_value, content_value, title_value))\n",
    "            n_inserted += 1\n",
    "        n += 1\n",
    "        if n % 1000 == 0 and n:\n",
    "            print(\"reading row:\", n, \"inserted\", n_inserted, \"rows\\r\", end = '')\n",
    "conn.commit()  # Commit changes to the database file\n",
    "print()\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4688cc96",
   "metadata": {},
   "source": [
    "# Run in sqlite:\n",
    "\n",
    "```\n",
    "CREATE TABLE nodup AS\n",
    "SELECT id, type, content, title, RANDOM() AS rnd\n",
    "FROM data\n",
    "GROUP BY content\n",
    "ORDER BY rnd;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f773fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put back into csv\n",
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "# Set the batch size for reading the data from the SQLite table\n",
    "batch_size = 500000\n",
    "database_path = 'temp_database.db'\n",
    "output_file = 'shuffled_fake_reliable.csv'\n",
    "# Connect to the database and create a cursor\n",
    "conn = sqlite3.connect(database_path)\n",
    "c = conn.cursor()\n",
    "\n",
    "# Execute a SELECT statement to get the column names\n",
    "c.execute(\"SELECT id, type, content, title FROM nodup LIMIT 0\")\n",
    "column_names = [description[0] for description in c.description]\n",
    "print('column names:', column_names)\n",
    "\n",
    "# Open the output file and write the header row\n",
    "with open(output_file, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(column_names)\n",
    "\n",
    "    # Execute a SELECT statement to read the data from the table in batches\n",
    "    offset = 0\n",
    "    while True:\n",
    "        c.execute(f\"SELECT id, type, content, title FROM nodup LIMIT {batch_size} OFFSET {offset}\")\n",
    "        rows = c.fetchall()\n",
    "\n",
    "        # Break the loop if no more rows are returned\n",
    "        if not rows:\n",
    "            break\n",
    "\n",
    "        # Write the rows to the output file\n",
    "        writer.writerows(rows)\n",
    "\n",
    "        # Increment the offset to read the next batch of rows\n",
    "        offset += batch_size\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b79159",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parquetise sql filtered data\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "csv_file_path = 'shuffled_fake_reliable.csv'\n",
    "parquet_file_prefix = 'cleaned_file.parquet/file_'\n",
    "parquet_file_suffix = '.parquet'\n",
    "\n",
    "column_dtypes = {\n",
    "    \"id\": int,\n",
    "    \"type\": object,\n",
    "    \"content\": object,\n",
    "    \"title\": object\n",
    "}\n",
    "#desired_columns = ['id', 'content', 'type']\n",
    "chunk_size = 50000  # number of rows per chunk\n",
    "\n",
    "# Iterate over the CSV file in chunks and write each chunk to a separate Parquet file\n",
    "for i, chunk in enumerate(pd.read_csv(csv_file_path, chunksize=chunk_size, dtype=column_dtypes)):\n",
    "    # Filter rows based on the \"type\" column\n",
    "    #chunk = chunk.query('type == \"fake\" or type == \"reliable\"').copy()\n",
    "        \n",
    "    # Create the filename for the current chunk\n",
    "    ident = str(i).zfill(4)\n",
    "    parquet_file_path = parquet_file_prefix + ident + parquet_file_suffix\n",
    "\n",
    "    # Write the current chunk to a Parquet file\n",
    "    table = pa.Table.from_pandas(chunk)\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "    print(\"\\rchunk\", ident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f579e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## But this\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import save_npz\n",
    "import contractions\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Custom tokenizer with lemmatization\n",
    "def custom_tokenizer(text):\n",
    "    # Expand contractions\n",
    "    expanded_text = contractions.fix(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    lower_text = expanded_text.lower()\n",
    "    \n",
    "    # Tokenize with RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\b[a-z]+\\b')\n",
    "    tokenized_words = tokenizer.tokenize(lower_text)\n",
    "\n",
    "    # Load English stopwords\n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in tokenized_words if word not in stopwords_set]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    \n",
    "    return lemmatized_words\n",
    "\n",
    "label_mapping = {'fake': 1, 'reliable': 0}\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, max_features=2**12, use_idf=False)\n",
    "\n",
    "# Directory containing your Parquet files\n",
    "parquet_directory = 'cleaned_file.parquet/'\n",
    "\n",
    "numpy_directory = 'numpyfiler/'\n",
    "\n",
    "# First pass: Fit the TfidfVectorizer on the entire dataset\n",
    "for parquet_file in sorted(glob.glob(os.path.join(parquet_directory, '*.parquet'))):\n",
    "    print(\"first pass: Parsing\", str(parquet_file))\n",
    "    df = pd.read_parquet(parquet_file, engine='pyarrow')\n",
    "    tfidf_vectorizer.fit(df['content'])\n",
    "\n",
    "# Update the TfidfVectorizer to use IDF\n",
    "tfidf_vectorizer.use_idf = True\n",
    "\n",
    "# Initialize counters for training and test set file indices\n",
    "train_idx = 0\n",
    "test_idx = 0\n",
    "\n",
    "reliable_factor = 0.426192247178894\n",
    "\n",
    "# Second pass: Transform the content using the TfidfVectorizer and save it incrementally to separate files\n",
    "for parquet_file in sorted(glob.glob(os.path.join(parquet_directory, '*.parquet'))):\n",
    "    print(\"second pass: parsing\", str(parquet_file))\n",
    "    df = pd.read_parquet(parquet_file, engine='pyarrow')\n",
    "    labels = np.array(df['type'])\n",
    "\n",
    "    # Split the current chunk of data into training and testing sets\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Separate the training chunk into fake and reliable classes\n",
    "    train_fake_df = train_df[train_df['type'] == 'fake']\n",
    "    train_reliable_df = train_df[train_df['type'] == 'reliable']\n",
    "\n",
    "    # Multiply the number of reliable samples in the training chunk by the factor\n",
    "    train_reliable_df_downsampled = train_reliable_df.sample(n=round(len(train_reliable_df) * reliable_factor), replace=False, random_state=42)\n",
    "\n",
    "    # Concatenate the balanced training chunk\n",
    "    train_balanced_df = pd.concat([train_fake_df, train_reliable_df_downsampled], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "    # Transform the content for the balanced training chunk and testing chunk\n",
    "    X_train_balanced = tfidf_vectorizer.transform(train_balanced_df['content'])\n",
    "    X_test_chunk = tfidf_vectorizer.transform(test_df['content'])\n",
    "\n",
    "    # Create labels array for the balanced training chunk and testing chunk\n",
    "    y_train_balanced = np.array([label_mapping[label] for label in train_balanced_df['type']])\n",
    "    y_test_chunk = np.array([label_mapping[label] for label in test_df['type']])\n",
    "\n",
    "    # Save the training data as .npz files\n",
    "    save_npz(os.path.join(numpy_directory, f'X_train_{train_idx:04d}.npz'), X_train_balanced)\n",
    "    np.save(os.path.join(numpy_directory, f'y_train_{train_idx:04d}.npy'), y_train_balanced)\n",
    "\n",
    "    # Save the test data as .npz files\n",
    "    save_npz(os.path.join(numpy_directory, f'X_test_{test_idx:04d}.npz'), X_test_chunk)\n",
    "    np.save(os.path.join(numpy_directory, f'y_test_{test_idx:04d}.npy'), y_test_chunk)\n",
    "\n",
    "    # Increment the counters\n",
    "    train_idx += 1\n",
    "    test_idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "numpy_dir = 'numpyfiler/'\n",
    "\n",
    "# Load the training data\n",
    "train_files = sorted(glob.glob(numpy_dir+'X_train_*.npz'))\n",
    "train_label_files = sorted(glob.glob(numpy_dir+'y_train_*.npy'))\n",
    "\n",
    "# Determine the input dimension from the first training file\n",
    "input_dim = load_npz(train_files[0]).shape[1]\n",
    "\n",
    "# Create a neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model incrementally using the saved training set files\n",
    "batch_size = 1536\n",
    "epochs = 1\n",
    "\n",
    "for x_file, y_file in zip(train_files, train_label_files):\n",
    "    print(\"training on\", x_file, y_file)\n",
    "    X_train_chunk = load_npz(x_file)\n",
    "    y_train_chunk = np.load(y_file, allow_pickle=True)\n",
    "\n",
    "    # Train the model in smaller batches\n",
    "    num_samples = X_train_chunk.shape[0]\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
    "\n",
    "            X_batch = X_train_chunk[start_idx:end_idx].todense()\n",
    "            y_batch = y_train_chunk[start_idx:end_idx]\n",
    "\n",
    "            loss, acc = model.train_on_batch(X_batch, y_batch)\n",
    "            print(f\" - Batch {batch_idx + 1}/{num_batches}: loss={loss:.4f}, accuracy={acc:.4f}\")\n",
    "            \n",
    "\n",
    "\n",
    "# Function to generate a unique filename\n",
    "def get_unique_filename(filename_prefix):\n",
    "    counter = 1\n",
    "    while os.path.exists(f'{filename_prefix}{counter}'):\n",
    "        counter += 1\n",
    "    return f'{filename_prefix}{counter}'\n",
    "\n",
    "# Save the model\n",
    "unique_filename = get_unique_filename('my_saved_model')\n",
    "model.save(unique_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data \n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "import glob\n",
    "from sklearn.metrics import accuracy_score\n",
    "batch_size = 128\n",
    "# Load the test set files\n",
    "numpy_dir = 'numpyfiler/'\n",
    "test_files = sorted(glob.glob(numpy_dir+'X_test_*.npz'))\n",
    "test_label_files = sorted(glob.glob(numpy_dir+'y_test_*.npy'))\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Make predictions on the test data\n",
    "for x_file, y_file in zip(test_files, test_label_files):\n",
    "    print(\"predicting on\", x_file, y_file)\n",
    "    X_test_chunk = load_npz(x_file)\n",
    "    y_test_chunk = np.load(y_file, allow_pickle=True)\n",
    "\n",
    "    # Process the test data in smaller batches\n",
    "    num_samples = X_test_chunk.shape[0]\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
    "\n",
    "        X_batch = X_test_chunk[start_idx:end_idx].todense()\n",
    "        y_batch = y_test_chunk[start_idx:end_idx]\n",
    "\n",
    "        # Get the predictions for this batch\n",
    "        y_pred_chunk = model.predict(X_batch)\n",
    "        \n",
    "        # Since the output activation is sigmoid, we need to threshold the predictions\n",
    "        y_pred_chunk = (y_pred_chunk > 0.5).astype(int).flatten()\n",
    "        \n",
    "        y_pred.extend(y_pred_chunk)\n",
    "        y_true.extend(y_batch)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e651e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "# Make confusion matrix\n",
    "confusion_matrix = metrics.confusion_matrix(y_true, y_pred, normalize=\"true\")\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(confusion_matrix, interpolation=\"nearest\", cmap=plt.cm.gray_r)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, format(confusion_matrix[i, j], '.2f'), horizontalalignment=\"center\", color=\"white\" if confusion_matrix[i, j] > 0.5 else \"black\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
