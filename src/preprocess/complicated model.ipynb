{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a0db1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 8528000 rows\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# Put id, type, column in temp database\n",
    "import sqlite3\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "random.seed(42)\n",
    "n = 0\n",
    "\n",
    "input_file = \"cleaned_file.csv\"\n",
    "#output_file = \"cleaned_file_shuffled.csv\"\n",
    "db_file = \"temp_database.db\"\n",
    "\n",
    "conn = sqlite3.connect(db_file)\n",
    "c = conn.cursor()\n",
    "c.execute(\"CREATE TABLE data (id TEXT, type TEXT, content TEXT, title TEXT, rnd INT);\")\n",
    "\n",
    "with open(input_file, \"r\") as f_in:\n",
    "    reader = csv.reader(f_in)\n",
    "    header = next(reader)  # Read the header\n",
    "\n",
    "    for row in reader:\n",
    "        id_value = row[1].replace(\"\\n\", \" \")\n",
    "        type_value = row[3].replace(\"\\n\", \" \")\n",
    "        content_value = row[5].replace(\"\\n\", \" \")\n",
    "        title_value = row[9].replace(\"\\n\", \" \")\n",
    "        rnd_value = int(random.uniform(-9223372036854775808, 9223372036854775807)) # sqlite max/min int values\n",
    "\n",
    "        c.execute(\"INSERT INTO data (id, type, content, title, rnd) VALUES (?, ?, ?, ?, ?)\", (id_value, type_value, content_value, title_value, rnd_value))\n",
    "        n += 1\n",
    "        if n % 1000 == 0 and n:\n",
    "            print(\"inserted\", n, \"rows\\r\", end = '')\n",
    "conn.commit()\n",
    "print()\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4a8ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "input_file = \"cleaned_file.csv\"\n",
    "#output_file = \"cleaned_file_shuffled.csv\"\n",
    "db_file = \"temp_database.db\"\n",
    "# Create new shuffled table\n",
    "conn = sqlite3.connect(db_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('''CREATE TABLE nodup AS\n",
    "             SELECT id, type, content, title, rnd\n",
    "             FROM data\n",
    "             GROUP BY content\n",
    "             ORDER BY rnd;''')\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "884af480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the old table to save space\n",
    "conn = sqlite3.connect(db_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('''DROP TABLE IF EXISTS data''')\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f773fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column names: ['id', 'type', 'content', 'title']\n"
     ]
    }
   ],
   "source": [
    "#put back into csv\n",
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "# Set the batch size for reading the data from the SQLite table\n",
    "batch_size = 400000\n",
    "database_path = db_file\n",
    "output_file = 'shuffled_dataset.csv'\n",
    "# Connect to the database and create a cursor\n",
    "conn = sqlite3.connect(database_path)\n",
    "c = conn.cursor()\n",
    "\n",
    "# Execute a SELECT statement to get the column names\n",
    "c.execute(\"SELECT id, type, content, title FROM nodup LIMIT 0\")\n",
    "column_names = [description[0] for description in c.description]\n",
    "print('column names:', column_names)\n",
    "\n",
    "# Open the output file and write the header row\n",
    "with open(output_file, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(column_names)\n",
    "\n",
    "    # Execute a SELECT statement to read the data from the table in batches\n",
    "    offset = 0\n",
    "    while True:\n",
    "        c.execute(f\"SELECT id, type, content, title FROM nodup LIMIT {batch_size} OFFSET {offset}\")\n",
    "        rows = c.fetchall()\n",
    "\n",
    "        # Break the loop if no more rows are returned\n",
    "        if not rows:\n",
    "            break\n",
    "\n",
    "        # Write the rows to the output file\n",
    "        writer.writerows(rows)\n",
    "\n",
    "        # Increment the offset to read the next batch of rows\n",
    "        offset += batch_size\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d1eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete database file\n",
    "import os\n",
    "os.remove(database_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77b79159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 0000\n",
      "chunk 0001\n",
      "chunk 0002\n",
      "chunk 0003\n",
      "chunk 0004\n",
      "chunk 0005\n",
      "chunk 0006\n",
      "chunk 0007\n",
      "chunk 0008\n",
      "chunk 0009\n",
      "chunk 0010\n",
      "chunk 0011\n",
      "chunk 0012\n",
      "chunk 0013\n",
      "chunk 0014\n",
      "chunk 0015\n",
      "chunk 0016\n",
      "chunk 0017\n",
      "chunk 0018\n",
      "chunk 0019\n",
      "chunk 0020\n",
      "chunk 0021\n",
      "chunk 0022\n",
      "chunk 0023\n",
      "chunk 0024\n",
      "chunk 0025\n",
      "chunk 0026\n",
      "chunk 0027\n",
      "chunk 0028\n",
      "chunk 0029\n",
      "chunk 0030\n",
      "chunk 0031\n",
      "chunk 0032\n",
      "chunk 0033\n",
      "chunk 0034\n",
      "chunk 0035\n",
      "chunk 0036\n",
      "chunk 0037\n",
      "chunk 0038\n",
      "chunk 0039\n",
      "chunk 0040\n",
      "chunk 0041\n",
      "chunk 0042\n",
      "chunk 0043\n",
      "chunk 0044\n",
      "chunk 0045\n",
      "chunk 0046\n",
      "chunk 0047\n",
      "chunk 0048\n",
      "chunk 0049\n",
      "chunk 0050\n",
      "chunk 0051\n",
      "chunk 0052\n",
      "chunk 0053\n",
      "chunk 0054\n",
      "chunk 0055\n",
      "chunk 0056\n",
      "chunk 0057\n",
      "chunk 0058\n",
      "chunk 0059\n",
      "chunk 0060\n",
      "chunk 0061\n",
      "chunk 0062\n",
      "chunk 0063\n",
      "chunk 0064\n",
      "chunk 0065\n",
      "chunk 0066\n",
      "chunk 0067\n",
      "chunk 0068\n",
      "chunk 0069\n",
      "chunk 0070\n",
      "chunk 0071\n",
      "chunk 0072\n",
      "chunk 0073\n",
      "chunk 0074\n",
      "chunk 0075\n",
      "chunk 0076\n",
      "chunk 0077\n",
      "chunk 0078\n",
      "chunk 0079\n",
      "chunk 0080\n",
      "chunk 0081\n",
      "chunk 0082\n",
      "chunk 0083\n",
      "chunk 0084\n",
      "chunk 0085\n",
      "chunk 0086\n",
      "chunk 0087\n",
      "chunk 0088\n",
      "chunk 0089\n",
      "chunk 0090\n",
      "chunk 0091\n",
      "chunk 0092\n",
      "chunk 0093\n",
      "chunk 0094\n",
      "chunk 0095\n",
      "chunk 0096\n",
      "chunk 0097\n",
      "chunk 0098\n",
      "chunk 0099\n",
      "chunk 0100\n",
      "chunk 0101\n",
      "chunk 0102\n",
      "chunk 0103\n",
      "chunk 0104\n",
      "chunk 0105\n",
      "chunk 0106\n",
      "chunk 0107\n",
      "chunk 0108\n",
      "chunk 0109\n",
      "chunk 0110\n",
      "chunk 0111\n",
      "chunk 0112\n",
      "chunk 0113\n",
      "chunk 0114\n",
      "chunk 0115\n",
      "chunk 0116\n",
      "chunk 0117\n",
      "chunk 0118\n",
      "chunk 0119\n",
      "chunk 0120\n",
      "chunk 0121\n",
      "chunk 0122\n",
      "chunk 0123\n",
      "chunk 0124\n"
     ]
    }
   ],
   "source": [
    "## Parquetise sql filtered data\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "csv_file_path = 'shuffled_dataset.csv'\n",
    "parquet_file_prefix = 'cleaned_file.parquet/file_'\n",
    "parquet_file_suffix = '.parquet'\n",
    "\n",
    "column_dtypes = {\n",
    "    \"id\": int,\n",
    "    \"type\": object,\n",
    "    \"content\": object,\n",
    "    \"title\": object\n",
    "}\n",
    "#desired_columns = ['id', 'content', 'type']\n",
    "chunk_size = 50000  # number of rows per chunk\n",
    "\n",
    "# Iterate over the CSV file in chunks and write each chunk to a separate Parquet file\n",
    "for i, chunk in enumerate(pd.read_csv(csv_file_path, chunksize=chunk_size, dtype=column_dtypes)):\n",
    "    # Filter rows based on the \"type\" column\n",
    "    #chunk = chunk.query('type == \"fake\" or type == \"reliable\"').copy()\n",
    "        \n",
    "    # Create the filename for the current chunk\n",
    "    ident = str(i).zfill(4)\n",
    "    parquet_file_path = parquet_file_prefix + ident + parquet_file_suffix\n",
    "\n",
    "    # Write the current chunk to a Parquet file\n",
    "    table = pa.Table.from_pandas(chunk)\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "    print(\"\\rchunk\", ident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "681c9e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-20 01:10:16.295204\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f579e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## But this\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import save_npz\n",
    "import contractions\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Custom tokenizer with lemmatization\n",
    "def custom_tokenizer(text):\n",
    "    # Expand contractions\n",
    "    #expanded_text = contractions.fix(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    lower_text = expanded_text.lower()\n",
    "    \n",
    "    # Tokenize with RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\b[a-z]+\\b')\n",
    "    tokenized_words = tokenizer.tokenize(lower_text)\n",
    "\n",
    "    # Load English stopwords\n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in tokenized_words if word not in stopwords_set]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    \n",
    "    return lemmatized_words\n",
    "\n",
    "label_mapping = {'fake': 1, 'reliable': 0}\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, max_features=2**12, use_idf=False)\n",
    "\n",
    "# Directory containing your Parquet files\n",
    "parquet_directory = 'cleaned_file.parquet/'\n",
    "\n",
    "numpy_directory = 'numpyfiler/'\n",
    "\n",
    "# First pass: Fit the TfidfVectorizer on the entire dataset\n",
    "for parquet_file in sorted(glob.glob(os.path.join(parquet_directory, '*.parquet'))):\n",
    "    print(\"first pass: Parsing\", str(parquet_file))\n",
    "    df = pd.read_parquet(parquet_file, engine='pyarrow')\n",
    "    tfidf_vectorizer.fit(df['content'])\n",
    "\n",
    "# Update the TfidfVectorizer to use IDF\n",
    "tfidf_vectorizer.use_idf = True\n",
    "\n",
    "# Initialize counters for training and test set file indices\n",
    "train_idx = 0\n",
    "test_idx = 0\n",
    "\n",
    "reliable_factor = 0.426192247178894\n",
    "\n",
    "# Second pass: Transform the content using the TfidfVectorizer and save it incrementally to separate files\n",
    "for parquet_file in sorted(glob.glob(os.path.join(parquet_directory, '*.parquet'))):\n",
    "    print(\"second pass: parsing\", str(parquet_file))\n",
    "    df = pd.read_parquet(parquet_file, engine='pyarrow')\n",
    "    labels = np.array(df['type'])\n",
    "\n",
    "    # Split the current chunk of data into training and testing sets\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Separate the training chunk into fake and reliable classes\n",
    "    train_fake_df = train_df[train_df['type'] == 'fake']\n",
    "    train_reliable_df = train_df[train_df['type'] == 'reliable']\n",
    "\n",
    "    # Multiply the number of reliable samples in the training chunk by the factor\n",
    "    train_reliable_df_downsampled = train_reliable_df.sample(n=round(len(train_reliable_df) * reliable_factor), replace=False, random_state=42)\n",
    "\n",
    "    # Concatenate the balanced training chunk\n",
    "    train_balanced_df = pd.concat([train_fake_df, train_reliable_df_downsampled], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "    # Transform the content for the balanced training chunk and testing chunk\n",
    "    X_train_balanced = tfidf_vectorizer.transform(train_balanced_df['content'])\n",
    "    X_test_chunk = tfidf_vectorizer.transform(test_df['content'])\n",
    "\n",
    "    # Create labels array for the balanced training chunk and testing chunk\n",
    "    y_train_balanced = np.array([label_mapping[label] for label in train_balanced_df['type']])\n",
    "    y_test_chunk = np.array([label_mapping[label] for label in test_df['type']])\n",
    "\n",
    "    # Save the training data as .npz files\n",
    "    save_npz(os.path.join(numpy_directory, f'X_train_{train_idx:04d}.npz'), X_train_balanced)\n",
    "    np.save(os.path.join(numpy_directory, f'y_train_{train_idx:04d}.npy'), y_train_balanced)\n",
    "\n",
    "    # Save the test data as .npz files\n",
    "    save_npz(os.path.join(numpy_directory, f'X_test_{test_idx:04d}.npz'), X_test_chunk)\n",
    "    np.save(os.path.join(numpy_directory, f'y_test_{test_idx:04d}.npy'), y_test_chunk)\n",
    "\n",
    "    # Increment the counters\n",
    "    train_idx += 1\n",
    "    test_idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "numpy_dir = 'numpyfiler/'\n",
    "\n",
    "# Load the training data\n",
    "train_files = sorted(glob.glob(numpy_dir+'X_train_*.npz'))\n",
    "train_label_files = sorted(glob.glob(numpy_dir+'y_train_*.npy'))\n",
    "\n",
    "# Determine the input dimension from the first training file\n",
    "input_dim = load_npz(train_files[0]).shape[1]\n",
    "\n",
    "# Create a neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model incrementally using the saved training set files\n",
    "batch_size = 1536\n",
    "epochs = 1\n",
    "\n",
    "for x_file, y_file in zip(train_files, train_label_files):\n",
    "    print(\"training on\", x_file, y_file)\n",
    "    X_train_chunk = load_npz(x_file)\n",
    "    y_train_chunk = np.load(y_file, allow_pickle=True)\n",
    "\n",
    "    # Train the model in smaller batches\n",
    "    num_samples = X_train_chunk.shape[0]\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
    "\n",
    "            X_batch = X_train_chunk[start_idx:end_idx].todense()\n",
    "            y_batch = y_train_chunk[start_idx:end_idx]\n",
    "\n",
    "            loss, acc = model.train_on_batch(X_batch, y_batch)\n",
    "            print(f\" - Batch {batch_idx + 1}/{num_batches}: loss={loss:.4f}, accuracy={acc:.4f}\")\n",
    "            \n",
    "\n",
    "\n",
    "# Function to generate a unique filename\n",
    "def get_unique_filename(filename_prefix):\n",
    "    counter = 1\n",
    "    while os.path.exists(f'{filename_prefix}{counter}'):\n",
    "        counter += 1\n",
    "    return f'{filename_prefix}{counter}'\n",
    "\n",
    "# Save the model\n",
    "unique_filename = get_unique_filename('my_saved_model')\n",
    "model.save(unique_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data \n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "import glob\n",
    "from sklearn.metrics import accuracy_score\n",
    "batch_size = 128\n",
    "# Load the test set files\n",
    "numpy_dir = 'numpyfiler/'\n",
    "test_files = sorted(glob.glob(numpy_dir+'X_test_*.npz'))\n",
    "test_label_files = sorted(glob.glob(numpy_dir+'y_test_*.npy'))\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Make predictions on the test data\n",
    "for x_file, y_file in zip(test_files, test_label_files):\n",
    "    print(\"predicting on\", x_file, y_file)\n",
    "    X_test_chunk = load_npz(x_file)\n",
    "    y_test_chunk = np.load(y_file, allow_pickle=True)\n",
    "\n",
    "    # Process the test data in smaller batches\n",
    "    num_samples = X_test_chunk.shape[0]\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
    "\n",
    "        X_batch = X_test_chunk[start_idx:end_idx].todense()\n",
    "        y_batch = y_test_chunk[start_idx:end_idx]\n",
    "\n",
    "        # Get the predictions for this batch\n",
    "        y_pred_chunk = model.predict(X_batch)\n",
    "        \n",
    "        # Since the output activation is sigmoid, we need to threshold the predictions\n",
    "        y_pred_chunk = (y_pred_chunk > 0.5).astype(int).flatten()\n",
    "        \n",
    "        y_pred.extend(y_pred_chunk)\n",
    "        y_true.extend(y_batch)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e651e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "# Make confusion matrix\n",
    "confusion_matrix = metrics.confusion_matrix(y_true, y_pred, normalize=\"true\")\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(confusion_matrix, interpolation=\"nearest\", cmap=plt.cm.gray_r)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, format(confusion_matrix[i, j], '.2f'), horizontalalignment=\"center\", color=\"white\" if confusion_matrix[i, j] > 0.5 else \"black\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
