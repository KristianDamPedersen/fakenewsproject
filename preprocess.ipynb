{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17906c82",
   "metadata": {},
   "source": [
    "# Take raw (Carriage return removed from bash script) csv, and assign each row a random number for shuffling, and put in sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0db1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put id, type, column in temp database\n",
    "import sqlite3\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "random.seed(42)\n",
    "n = 0\n",
    "\n",
    "path = 'data/'\n",
    "input_file = path + 'cr_removed.csv'\n",
    "\n",
    "db_file = path + 'temp_database.db'\n",
    "\n",
    "conn = sqlite3.connect(db_file)\n",
    "c = conn.cursor()\n",
    "c.execute(\"CREATE TABLE data (id TEXT, type TEXT, content TEXT, title TEXT, rnd INT);\")\n",
    "\n",
    "with open(input_file, \"r\") as f_in:\n",
    "    reader = csv.reader(f_in)\n",
    "    header = next(reader)  # Read the header\n",
    "\n",
    "    for row in reader:\n",
    "        id_value = row[1].replace(\"\\n\", \" \")\n",
    "        type_value = row[3].replace(\"\\n\", \" \")\n",
    "        content_value = row[5].replace(\"\\n\", \" \")\n",
    "        title_value = row[9].replace(\"\\n\", \" \")\n",
    "        rnd_value = int(\n",
    "            random.uniform(-9223372036854775808, 9223372036854775807))  # sqlite max/min int values\n",
    "\n",
    "        c.execute(\n",
    "            \"INSERT INTO data (id, type, content, title, rnd) VALUES (?, ?, ?, ?, ?)\",\n",
    "            (id_value, type_value, content_value, title_value, rnd_value),\n",
    "        )\n",
    "        n += 1\n",
    "        if n % 1000 == 0 and n:\n",
    "            print(\"inserted\", n, \"rows\\r\", end=\"\")\n",
    "conn.commit()\n",
    "print()\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe966f3",
   "metadata": {},
   "source": [
    "# Shuffle and deduplicate database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a8ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "conn = sqlite3.connect(db_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\n",
    "    \"\"\"CREATE TABLE nodup AS\n",
    "             SELECT id, type, content, title, rnd\n",
    "             FROM data\n",
    "             GROUP BY content\n",
    "             ORDER BY rnd;\"\"\"\n",
    ")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be3169",
   "metadata": {},
   "source": [
    "# Delete the old database table to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884af480",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(db_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"\"\"DROP TABLE IF EXISTS data\"\"\")\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81ff48",
   "metadata": {},
   "source": [
    "# load shuffled and deduped data back from database into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f773fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "# Set the batch size for reading the data from the SQLite table\n",
    "batch_size = 400000\n",
    "\n",
    "output_file = path + \"shuffled_dataset.csv\"\n",
    "conn = sqlite3.connect(db_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "# Execute a SELECT statement to get the column names\n",
    "c.execute(\"SELECT id, type, content, title FROM nodup LIMIT 0\")\n",
    "column_names = [description[0] for description in c.description]\n",
    "print(\"column names:\", column_names)\n",
    "\n",
    "# Open the output file and write only the header row\n",
    "with open(output_file, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(column_names)\n",
    "\n",
    "    # Execute a SELECT statement to read the data from the table in batches\n",
    "    offset = 0\n",
    "    while True:\n",
    "        c.execute(\n",
    "            f\"SELECT id, type, content, title FROM nodup LIMIT {batch_size} OFFSET {offset}\"\n",
    "        )\n",
    "        rows = c.fetchall()\n",
    "\n",
    "        # Break the loop if no more rows are returned\n",
    "        if not rows:\n",
    "            break\n",
    "\n",
    "        # Write the rows to the output file\n",
    "        writer.writerows(rows)\n",
    "\n",
    "        # Increment the offset to read the next batch of rows\n",
    "        offset += batch_size\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68fdd34",
   "metadata": {},
   "source": [
    "# Delete entire database, to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove(db_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b6b44e",
   "metadata": {},
   "source": [
    "# Parquetise csv in chunks of 50000 rows (approx 100M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b79159",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parquetise sql filtered data\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "csv_file_path = output_file\n",
    "parquet_file_prefix = path + \"shuffled_deduped.parquet/file_\"\n",
    "parquet_file_suffix = \".parquet\"\n",
    "\n",
    "column_dtypes = {\"id\": int, \"type\": object, \"content\": object, \"title\": object}\n",
    "# desired_columns = ['id', 'content', 'type']\n",
    "chunk_size = 50000  # number of rows per chunk\n",
    "\n",
    "# Iterate over the CSV file in chunks and write each chunk to a separate Parquet file\n",
    "for i, chunk in enumerate(\n",
    "    pd.read_csv(csv_file_path, chunksize=chunk_size, dtype=column_dtypes)\n",
    "):\n",
    "    # Filter rows based on the \"type\" column\n",
    "    # chunk = chunk.query('type == \"fake\" or type == \"reliable\"').copy()\n",
    "\n",
    "    # Create the filename for the current chunk\n",
    "    ident = str(i).zfill(4)\n",
    "    parquet_file_path = parquet_file_prefix + ident + parquet_file_suffix\n",
    "\n",
    "    # Write the current chunk to a Parquet file\n",
    "    table = pa.Table.from_pandas(chunk)\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "    print(\"chunk\", ident,'\\r', end='')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "file_name = path + \"shuffled_deduped.parquet\"\n",
    "df = pd.read_parquet(file_name, columns=[\"type\", \"content\"])\n",
    "# Filter out the 'empty' and 'unknown' types\n",
    "df = df[df[\"type\"].isin([\"\", \"unknown\"]) == False]\n",
    "df_size = df.shape[0]\n",
    "\n",
    "n_non_reliable = 3897597\n",
    "n_reliable = 1808242\n",
    "downsample_factor = n_reliable / n_non_reliable\n",
    "\n",
    "\n",
    "train_size = round(df_size * 0.8)\n",
    "val_size = (df_size - train_size) // 2\n",
    "test_size = df_size - train_size - val_size\n",
    "\n",
    "\n",
    "def dicethrow(thistype):\n",
    "    if thistype == \"reliable\":\n",
    "        return True\n",
    "    rand = random.random()\n",
    "    if rand > downsample_factor:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Generate y values for models.\n",
    "df[\"class\"] = df[\"type\"] == \"reliable\"\n",
    "\n",
    "train_data = df.iloc[:train_size]\n",
    "val_data = df.iloc[train_size : train_size + val_size]\n",
    "test_data = df.iloc[train_size + val_size :]\n",
    "\n",
    "train_data = train_data[train_data[\"type\"].map(dicethrow)]\n",
    "\n",
    "\n",
    "# WRITE PARQUETS\n",
    "def parquetise(df, path: str, set_type: str, chunk_size=100000):\n",
    "    \"\"\"Parquetise some dataframe\n",
    "    input: df (dataframe),\n",
    "    path: The relative path, such as 'many_parquet_files_here.parquet/'\n",
    "    set_type, string. typically \"train\", \"val\" or \"test\",\n",
    "    chunk_size (optional): the number of rows per chunk\"\"\"\n",
    "\n",
    "    file_n = 0\n",
    "    df_length = len(df)\n",
    "    for i in range(0, df_length, chunk_size):\n",
    "        ident = str(file_n).zfill(6)\n",
    "\n",
    "        chunk_df = df.iloc[i : min(i + chunk_size, df_length)]\n",
    "        filename = f\"{set_type}_{ident}.parquet\"\n",
    "        print(\"Writing:\", filename, \"\\r\", end=\"\")\n",
    "        chunk_df.to_parquet(path + filename, index=False)\n",
    "        file_n += 1\n",
    "    print()\n",
    "    print(\"done!\")\n",
    "\n",
    "\n",
    "parquetise(train_data, path+\"train.parquet/\", \"train\")\n",
    "parquetise(val_data, path+\"val.parquet/\", \"val\")\n",
    "parquetise(test_data, path+\"test.parquet/\", \"test\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
