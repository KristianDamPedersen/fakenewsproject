{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04241741",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parquetise only fake and reliable news\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "csv_file_path = \"cleaned_file.csv\"\n",
    "parquet_file_prefix = \"cleaned_file.parquet/file_\"\n",
    "parquet_file_suffix = \".parquet\"\n",
    "\n",
    "column_dtypes = {\"id\": int, \"type\": object, \"content\": object}\n",
    "desired_columns = [\"id\", \"content\", \"type\"]\n",
    "chunk_size = 100000  # number of rows per chunk\n",
    "\n",
    "# Iterate over the CSV file in chunks and write each chunk to a separate Parquet file\n",
    "for i, chunk in enumerate(\n",
    "    pd.read_csv(csv_file_path, chunksize=chunk_size, dtype=column_dtypes)\n",
    "):\n",
    "    # Filter rows based on the \"type\" column\n",
    "    chunk = chunk.query('type == \"fake\" or type == \"reliable\"').copy()\n",
    "\n",
    "    if not chunk.empty:\n",
    "        # Create the filename for the current chunk\n",
    "        ident = str(i).zfill(4)\n",
    "        parquet_file_path = parquet_file_prefix + ident + parquet_file_suffix\n",
    "\n",
    "        # Write the current chunk to a Parquet file\n",
    "        table = pa.Table.from_pandas(chunk[desired_columns])\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "        print(\"\\rchunk\", ident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca744dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "\n",
    "# Custom tokenizer with lemmatization\n",
    "def custom_tokenizer(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "label_mapping = {\"fake\": 1, \"reliable\": 0}\n",
    "\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer, max_features=2**18, use_idf=False\n",
    ")\n",
    "\n",
    "# Directory containing your Parquet files\n",
    "parquet_directory = \"cleaned_file.parquet/\"\n",
    "\n",
    "# First pass: Fit the TfidfVectorizer on the entire dataset\n",
    "for parquet_file in glob.glob(os.path.join(parquet_directory, \"*.parquet\")):\n",
    "    print(\"first pass: Parsing\", str(parquet_file))\n",
    "    df = pd.read_parquet(parquet_file, engine=\"pyarrow\")\n",
    "    tfidf_vectorizer.fit(df[\"content\"])\n",
    "\n",
    "# Update the TfidfVectorizer to use IDF\n",
    "tfidf_vectorizer.use_idf = True\n",
    "\n",
    "# Initialize counters for training and test set file indices\n",
    "train_idx = 0\n",
    "test_idx = 0\n",
    "\n",
    "# Second pass: Transform the content using the TfidfVectorizer and save it incrementally to separate files\n",
    "for parquet_file in glob.glob(os.path.join(parquet_directory, \"*.parquet\")):\n",
    "    print(\"second pass: parsing\", str(parquet_file))\n",
    "    df = pd.read_parquet(parquet_file, engine=\"pyarrow\")\n",
    "\n",
    "    # Transform the content and create labels array\n",
    "    vectorized_contents = tfidf_vectorizer.transform(df[\"content\"])\n",
    "    labels = np.array(df[\"type\"])\n",
    "\n",
    "    # Split the current chunk of data into training and testing sets\n",
    "    X_train_chunk, X_test_chunk, y_train_chunk, y_test_chunk = train_test_split(\n",
    "        vectorized_contents, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Encode labels\n",
    "    y_train_chunk = np.array([label_mapping[label] for label in y_train_chunk])\n",
    "    y_test_chunk = np.array([label_mapping[label] for label in y_test_chunk])\n",
    "\n",
    "    # Saving the data\n",
    "    # Save the training and testing sets as binary files\n",
    "    save_npz(f\"X_train_{train_idx:04d}.npz\", X_train_chunk)\n",
    "    save_npz(f\"X_test_{test_idx:04d}.npz\", X_test_chunk)\n",
    "    np.save(f\"y_train_{train_idx:04d}.npy\", y_train_chunk)\n",
    "    np.save(f\"y_test_{test_idx:04d}.npy\", y_test_chunk)\n",
    "\n",
    "    # Increment the counters\n",
    "    train_idx += 1\n",
    "    test_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6825054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the training data\n",
    "train_files = sorted(glob.glob(\"X_train_*.npz\"))\n",
    "train_label_files = sorted(glob.glob(\"y_train_*.npy\"))\n",
    "\n",
    "# Determine the input dimension from the first training file\n",
    "input_dim = load_npz(train_files[0]).shape[1]\n",
    "\n",
    "# Create a neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation=\"relu\"))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train the model incrementally using the saved training set files\n",
    "batch_size = 1536\n",
    "epochs = 1\n",
    "\n",
    "for x_file, y_file in zip(train_files, train_label_files):\n",
    "    print(\"training on\", x_file, y_file)\n",
    "    X_train_chunk = load_npz(x_file)\n",
    "    y_train_chunk = np.load(y_file, allow_pickle=True)\n",
    "\n",
    "    # Train the model in smaller batches\n",
    "    num_samples = X_train_chunk.shape[0]\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
    "\n",
    "            X_batch = X_train_chunk[start_idx:end_idx].todense()\n",
    "            y_batch = y_train_chunk[start_idx:end_idx]\n",
    "\n",
    "            loss, acc = model.train_on_batch(X_batch, y_batch)\n",
    "            print(\n",
    "                f\" - Batch {batch_idx + 1}/{num_batches}: loss={loss:.4f}, accuracy={acc:.4f}\"\n",
    "            )\n",
    "\n",
    "model.save(\"my_saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f3d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6714ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = load_model(\"my_saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a068ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data (doesn't work)\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "import glob\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "batch_size = 128\n",
    "# Load the test set files\n",
    "test_files = sorted(glob.glob(\"X_test_*.npz\"))\n",
    "test_label_files = sorted(glob.glob(\"y_test_*.npy\"))\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Make predictions on the test data\n",
    "for x_file, y_file in zip(test_files, test_label_files):\n",
    "    print(\"predicting on\", x_file, y_file)\n",
    "    X_test_chunk = load_npz(x_file)\n",
    "    y_test_chunk = np.load(y_file, allow_pickle=True)\n",
    "\n",
    "    # Process the test data in smaller batches\n",
    "    num_samples = X_test_chunk.shape[0]\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
    "\n",
    "        X_batch = X_test_chunk[start_idx:end_idx].todense()\n",
    "        y_batch = y_test_chunk[start_idx:end_idx]\n",
    "\n",
    "        # Get the predictions for this batch\n",
    "        y_pred_chunk = model.predict(X_batch)\n",
    "\n",
    "        # Since the output activation is sigmoid, we need to threshold the predictions\n",
    "        y_pred_chunk = (y_pred_chunk > 0.5).astype(int).flatten()\n",
    "\n",
    "        y_pred.extend(y_pred_chunk)\n",
    "        y_true.extend(y_batch)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
